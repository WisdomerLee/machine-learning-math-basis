# 🎯 Logistic Regression이란?

Logistic Regression은 이름에 *회귀(Regression)*가 들어가 있지만, 실제로는 **분류(Classification)** 문제를 푸는 데 사용되는 알고리즘입니다.

주로 **이진 분류(Binary Classification)** — 예를 들어, 이메일이 스팸인지 아닌지를 분류하는 데 활용됩니다.

---

## 🧠 핵심 아이디어

선형 회귀(Linear Regression)는 입력값에 가중치를 곱하고 더한 결과를 그대로 출력합니다. 하지만 이 방식은 분류 문제에 적합하지 않습니다. 왜냐하면 출력값이 **0부터 1 사이의 확률**이어야 하기 때문이죠.

그래서 로지스틱 회귀는 **시그모이드 함수(Sigmoid Function)**를 사용해서 예측값을 확률로 바꿉니다.

---

## ⚙️ 수식으로 이해하기

### 1. 입력과 가중치의 선형 결합

z = w1 * x1 + w2 * x2 + ... + wn * xn + b


- `x1, x2, ..., xn` : 입력 데이터
- `w1, w2, ..., wn` : 가중치 (weight)
- `b` : 편향 (bias)

### 2. 시그모이드 함수 적용

σ(z) = 1 / (1 + e^(-z))


- 이 함수는 어떤 실수 값도 0과 1 사이의 값으로 바꿔줍니다.
- 출력은 "양성 클래스일 확률"로 해석할 수 있습니다.

---

## 📊 분류 기준

출력된 확률이 `0.5`보다 크면 **양성(positive)**, 작으면 **음성(negative)**으로 분류합니다.

if σ(z) >= 0.5: 예측 = 1 (양성 클래스) else: 예측 = 0 (음성 클래스)


---

## 🏋️‍♂️ 학습 방법 (Training)

Logistic Regression은 **비용 함수(Cost Function)**를 최소화하는 방향으로 가중치들을 조정합니다.

사용되는 대표적인 비용 함수는 **이진 교차 엔트로피(Binary Cross Entropy)**입니다.


Cost = -[y*log(p) + (1-y)*log(1-p)]

- `y`는 실제 정답 (0 또는 1)
- `p`는 예측 확률 (σ(z))
- 이 값을 최소화하는 방향으로 경사 하강법(Gradient Descent)으로 학습합니다.

---

## ✅ 사용 예시

- 이메일이 스팸인지 아닌지
- 환자가 질병을 가졌는지 아닌지
- 고객이 상품을 구매할지 아닐지

---

## 📌 정리

| 항목 | 설명 |
|------|------|
| 목적 | 이진 분류 |
| 출력 | 0~1 사이의 확률 |
| 핵심 함수 | 시그모이드 함수 |
| 학습 방법 | 비용 함수 최소화 (경사 하강법 사용) |

---

## 💡 참고 이미지 (직관적 이해)

> 예: 시그모이드 함수 그래프

   |
 1 |                _______
   |               /
 0.5 |-----------/
   |          /
 0 |________/
   |
     -----------------

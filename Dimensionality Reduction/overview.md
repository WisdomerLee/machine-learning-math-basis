# ì°¨ì› ì¶•ì†Œë€?
> **ê³ ì°¨ì› ë°ì´í„°ë¥¼ ë” ë‚®ì€ ì°¨ì›ì˜ ê³µê°„ìœ¼ë¡œ ë³€í™˜**í•˜ëŠ” ê²ƒ.  
> ì˜ˆ: 1000ê°œì˜ íŠ¹ì„±ì„ ê°€ì§„ ë°ì´í„°ë¥¼ 2~50ê°œ ì •ë„ì˜ íŠ¹ì„±ìœ¼ë¡œ ì¤„ì„.

---

# ì™œ ì°¨ì› ì¶•ì†Œê°€ í•„ìš”í• ê¹Œ?
1. **ê³„ì‚° íš¨ìœ¨ í–¥ìƒ**: ì…ë ¥ ì°¨ì›ì´ ì¤„ì–´ë“¤ë©´ ì—°ì‚°ëŸ‰ â†“
2. **ë…¸ì´ì¦ˆ ì œê±°**: ë¶ˆí•„ìš”í•œ ì •ë³´ ì œê±° â†’ ë” ì¢‹ì€ ì¼ë°˜í™” ì„±ëŠ¥
3. **ì‹œê°í™” ê°€ëŠ¥**: 2D ë˜ëŠ” 3Dë¡œ ë³€í™˜í•´ì„œ ë°ì´í„° êµ¬ì¡° ì´í•´
4. **ì°¨ì›ì˜ ì €ì£¼(Curse of Dimensionality) ì™„í™”**  
   â†’ ì°¨ì›ì´ ì»¤ì§ˆìˆ˜ë¡ í•™ìŠµì´ ì–´ë ¤ì›Œì§€ëŠ” ë¬¸ì œë¥¼ í•´ê²°

---

# ì°¨ì› ì¶•ì†Œ ë°©ë²• 2ê°€ì§€ ë¶„ë¥˜
1. **ì„ í˜•(LINEAR)**: ë°ì´í„°ê°€ ì„ í˜•ì ìœ¼ë¡œ ë¶„í¬í•œë‹¤ê³  ê°€ì •
2. **ë¹„ì„ í˜•(NONLINEAR)**: ë³µì¡í•œ êµ¬ì¡°ì˜ ë°ì´í„°ë„ ë‹¤ë£° ìˆ˜ ìˆìŒ

---

# ëŒ€í‘œì ì¸ ì°¨ì› ì¶•ì†Œ ê¸°ë²•

## 1. ğŸ“˜ PCA (ì£¼ì„±ë¶„ ë¶„ì„, Principal Component Analysis)
- **ê°€ì¥ ë„ë¦¬ ì“°ì´ëŠ” ì„ í˜• ì°¨ì› ì¶•ì†Œ ê¸°ë²•**
- ë°ì´í„°ì˜ **ë¶„ì‚°ì´ ê°€ì¥ í° ë°©í–¥**ì„ ê¸°ì¤€ìœ¼ë¡œ ìƒˆ ì¶•(ì£¼ì„±ë¶„)ì„ ì¡ê³ , ê·¸ ì¶•ì— íˆ¬ì˜í•´ì„œ ì°¨ì›ì„ ì¤„ì„
- ğŸ’¡ í•µì‹¬: ìµœëŒ€í•œ ì •ë³´ë¥¼ ë³´ì¡´í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ì¶•ì„ íšŒì „ì‹œí‚¤ëŠ” ê²ƒ

ğŸ‘‰ ì‚¬ìš© ì˜ˆì‹œ: ì´ë¯¸ì§€ ì••ì¶•, ì‚¬ì „ ë°ì´í„° ì „ì²˜ë¦¬

---

## 2. ğŸ“• t-SNE (t-Distributed Stochastic Neighbor Embedding)
- ë¹„ì„ í˜• ê¸°ë²•, **ê³ ì°¨ì› ë°ì´í„°ë¥¼ 2D/3Dë¡œ ì‹œê°í™”**í•  ë•Œ íƒì›”
- ê³ ì°¨ì›ì—ì„œ ê°€ê¹Œìš´ ì ë“¤ì´ ë‚®ì€ ì°¨ì›ì—ì„œë„ ê°€ê¹ë„ë¡ ìœ ì§€
- ë‹¨ì : ê³„ì‚° ë¹„ìš© í¼, ìƒˆë¡œìš´ ìƒ˜í”Œ ì˜ˆì¸¡ ì–´ë ¤ì›€

ğŸ‘‰ ì‚¬ìš© ì˜ˆì‹œ: ì´ë¯¸ì§€, í…ìŠ¤íŠ¸ ì„ë² ë”© ê²°ê³¼ ì‹œê°í™”

---

## 3. ğŸ“— UMAP (Uniform Manifold Approximation and Projection)
- t-SNEë³´ë‹¤ ë¹ ë¥´ê³  ì¼ë°˜í™”ê°€ ì‰¬ìš´ ë¹„ì„ í˜• ë°©ë²•
- êµ°ì§‘ êµ¬ì¡° ë³´ì¡´ì´ ì˜ ë¨
- ğŸ’¡ ìµœê·¼ì—” t-SNEë³´ë‹¤ ë” ë§ì´ ì“°ì´ëŠ” ì¶”ì„¸

ğŸ‘‰ ì‚¬ìš© ì˜ˆì‹œ: í´ëŸ¬ìŠ¤í„°ë§ ì „ ì „ì²˜ë¦¬, ì‹œê°í™”

---

## 4. ğŸ“™ Autoencoder (ì˜¤í† ì¸ì½”ë”)
- ì‹ ê²½ë§ ê¸°ë°˜ ì°¨ì› ì¶•ì†Œ
- ì¸ì½”ë”(ì••ì¶•) â†’ ë””ì½”ë”(ë³µì›) êµ¬ì¡°
- **ì ì¬ ê³µê°„(latent space)**ì„ ì‚¬ìš©í•´ ë°ì´í„°ë¥¼ ì¶•ì†Œ
- í•™ìŠµ í•„ìš”, ë¹„ì„ í˜• í‘œí˜„ ê°€ëŠ¥

ğŸ‘‰ ì‚¬ìš© ì˜ˆì‹œ: ì´ìƒ íƒì§€, ì´ë¯¸ì§€ ì¬êµ¬ì„±, í…ìŠ¤íŠ¸ ì„ë² ë”©

---

# ë”¥ëŸ¬ë‹ì—ì„œì˜ í™œìš© ì˜ˆ
| ì ìš© ë¶„ì•¼        | ì°¨ì› ì¶•ì†Œ ì—­í•                       |
|------------------|-------------------------------------|
| ì´ë¯¸ì§€ ì²˜ë¦¬      | ê³ í•´ìƒë„ ì´ë¯¸ì§€ â†’ ë‚®ì€ ì°¨ì› ì••ì¶•    |
| í…ìŠ¤íŠ¸ ì„ë² ë”©    | BERT ì¶œë ¥ â†’ 2D ì‹œê°í™”               |
| ì´ìƒ íƒì§€        | ì˜¤í† ì¸ì½”ë” latent space ì‚¬ìš©         |
| í´ëŸ¬ìŠ¤í„°ë§       | UMAP í›„ K-means ë“± êµ°ì§‘í™” ì ìš©       |

---

# âœ… ì´í•´ë¥¼ ìœ„í•œ ê¸°ë³¸ ê°œë… ì •ë¦¬
- **ê³ ì°¨ì›**: íŠ¹ì„±(feature) ìˆ˜ê°€ ë§ì€ ë°ì´í„°
- **ë¶„ì‚°(Variance)**: ë°ì´í„°ì˜ í¼ì§ ì •ë„
- **ì„ í˜•ë³€í™˜**: í–‰ë ¬ ê³± ë“±ìœ¼ë¡œ ë°ì´í„°ë¥¼ íšŒì „/ë³€í˜•
- **ë¹„ì„ í˜• ë³€í™˜**: ê³¡ì„ ì²˜ëŸ¼ êµ¬ë¶€ëŸ¬ì§„ í˜•íƒœë¡œ ë°ì´í„° ë§¤í•‘
- **ì ì¬ ê³µê°„(latent space)**: ì¤‘ìš”í•œ ì •ë³´ë§Œ ë‹´ê¸´ ì¶•ì†Œëœ í‘œí˜„ ê³µê°„

---

# ê³µí†µ ì¤€ë¹„ ì½”ë“œ

```python
from sklearn.datasets import load_digits
import matplotlib.pyplot as plt

# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
digits = load_digits()
X = digits.data  # (1797, 64) ì´ë¯¸ì§€ ë°ì´í„°
y = digits.target  # (1797,) ë¼ë²¨

def plot_2d(X_2d, y, title):
    plt.figure(figsize=(8, 6))
    scatter = plt.scatter(X_2d[:, 0], X_2d[:, 1], c=y, cmap='tab10', s=15)
    plt.legend(*scatter.legend_elements(), title="Digits")
    plt.title(title)
    plt.show()
```

---

# 1. ğŸ“˜ PCA (ì„ í˜•)

```python
from sklearn.decomposition import PCA

# PCA ì ìš© (2ì°¨ì›)
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

# ì‹œê°í™”
plot_2d(X_pca, y, "PCA (2D)")
```

---

# 2. ğŸ“• t-SNE (ë¹„ì„ í˜•)

```python
from sklearn.manifold import TSNE

# t-SNE ì ìš© (2ì°¨ì›)
tsne = TSNE(n_components=2, perplexity=30, random_state=0)
X_tsne = tsne.fit_transform(X)

# ì‹œê°í™”
plot_2d(X_tsne, y, "t-SNE (2D)")
```

---

# 3. ğŸ“— UMAP (ë¹„ì„ í˜•)

```python
import umap.umap_ as umap

# UMAP ì ìš© (2ì°¨ì›)
umap_model = umap.UMAP(n_components=2, random_state=0)
X_umap = umap_model.fit_transform(X)

# ì‹œê°í™”
plot_2d(X_umap, y, "UMAP (2D)")
```

> ğŸ’¡ `pip install umap-learn` í•„ìš”

---

# 4. ğŸ“™ Autoencoder (ì‹ ê²½ë§ ê¸°ë°˜)

```python
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.optimizers import Adam
from sklearn.preprocessing import MinMaxScaler

# ë°ì´í„° ì •ê·œí™”
X_scaled = MinMaxScaler().fit_transform(X)

# ì˜¤í† ì¸ì½”ë” êµ¬ì„±
input_dim = X.shape[1]
encoding_dim = 2  # 2ì°¨ì› latent space

input_img = Input(shape=(input_dim,))
encoded = Dense(32, activation='relu')(input_img)
encoded = Dense(encoding_dim, activation='linear')(encoded)  # ì ì¬ê³µê°„
decoded = Dense(32, activation='relu')(encoded)
decoded = Dense(input_dim, activation='sigmoid')(decoded)

autoencoder = Model(input_img, decoded)
encoder = Model(input_img, encoded)

# í•™ìŠµ
autoencoder.compile(optimizer=Adam(), loss='mse')
autoencoder.fit(X_scaled, X_scaled, epochs=30, batch_size=64, verbose=0)

# ì¸ì½”ë”© ê²°ê³¼ ì¶”ì¶œ
X_ae = encoder.predict(X_scaled)

# ì‹œê°í™”
plot_2d(X_ae, y, "Autoencoder (2D)")
```

---

# ğŸ” ìš”ì•½

| ê¸°ë²•       | ë¼ì´ë¸ŒëŸ¬ë¦¬              | ì¥ì                  | ë‹¨ì                    |
|------------|--------------------------|----------------------|------------------------|
| PCA        | `sklearn`                | ë¹ ë¥´ê³  ì§ê´€ì         | ì„ í˜•ì„± í•œê³„            |
| t-SNE      | `sklearn`                | ì‹œê°í™”ì— ì í•©        | ëŠë¦¬ê³  ì¼ë°˜í™” ì–´ë ¤ì›€   |
| UMAP       | `umap-learn`             | ë¹ ë¥´ê³  êµ°ì§‘ ë³´ì¡´     | ì´ˆë§¤ê°œë³€ìˆ˜ ì˜í–¥ í¼      |
| Autoencoder| `tensorflow/keras`       | ë¹„ì„ í˜• ë³µì¡ íŒ¨í„´ í•™ìŠµ| í•™ìŠµ í•„ìš”, íŠœë‹ í•„ìš”    |

---

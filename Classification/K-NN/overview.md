# 🧭 K-Nearest Neighbors (K-NN)란?

K-최근접 이웃(K-Nearest Neighbors, K-NN)은 가장 **직관적이고 간단한 머신러닝 알고리즘** 중 하나로, 새로운 데이터가 주어졌을 때 **가장 가까운 이웃(K개)의 클래스를 참고해서 분류**합니다.

---

## 🧠 핵심 아이디어

> "비슷한 데이터는 비슷한 범주에 속한다!"

K-NN은 어떤 복잡한 수식을 이용하는 것이 아니라, 단순히 **거리 계산**을 통해 주변 데이터를 보고 결정을 내립니다.

---

## ⚙️ 작동 방식 (Step by Step)

1. **훈련 데이터를 저장**
   - 학습이라고 해서 모델이 가중치를 조정하는 게 아니라, 단순히 데이터를 저장만 합니다.

2. **새로운 데이터가 들어오면**
   - 이 데이터와 훈련 데이터 간의 거리를 계산합니다.

3. **가장 가까운 K개의 이웃을 찾고**
   - 거리 기준으로 가까운 데이터를 찾습니다.

4. **이웃들의 다수결로 분류**
   - K개의 이웃 중 가장 많이 등장한 클래스가 예측 결과입니다.

---

## 📏 거리 계산 방법

가장 많이 쓰는 방법: **유클리드 거리**

거리 = √[(x1 - x2)² + (y1 - y2)² + ...]

또는 상황에 따라 맨해튼 거리, 코사인 유사도 등을 사용할 수 있습니다.

---

## 🔢 K 값은 어떻게 정할까?

- `K=1`이면 가장 가까운 한 개만 참고 → 민감하고 노이즈에 취약
- `K`가 너무 크면 너무 많은 이웃을 참고 → 경계가 흐려짐

보통 **홀수**로 설정해서 다수결이 잘 되도록 합니다.  
적절한 K는 교차 검증(Cross Validation)으로 찾습니다.

---

## 📊 예시

> 아래와 같은 데이터를 보자

| 데이터 | 특성1 | 특성2 | 클래스 |
|--------|-------|-------|--------|
| A      | 1.0   | 2.0   | 🍎     |
| B      | 1.5   | 1.8   | 🍎     |
| C      | 3.0   | 3.5   | 🍌     |

이제 새로운 데이터 (2.0, 2.5)가 들어오면?

- A, B와 가까움 → 둘 다 🍎 → 예측 결과는 **🍎**

---

## 📦 장단점 요약

| 장점 | 단점 |
|------|------|
| 구현이 쉽고 직관적이다 | 계산량이 많다 (특히 데이터가 많을 때) |
| 학습 시간이 거의 없다 (Lazy Learning) | 차원의 저주에 취약 (고차원일수록 거리 계산이 부정확) |
| 다양한 분류 문제에 적용 가능 | 이상치나 노이즈에 민감 |

---

## ✅ 사용 예시

- 손글씨 숫자 분류 (예: MNIST)
- 영화/상품 추천 (비슷한 사용자의 취향 찾기)
- 패턴 인식, 얼굴 인식 등

---

## 💡 정리

| 항목 | 설명 |
|------|------|
| 목적 | 분류 (또는 회귀도 가능) |
| 학습 방식 | 훈련 데이터를 저장만 함 |
| 예측 방식 | 거리 기준으로 K개의 이웃을 찾아 다수결 |
| 주요 파라미터 | K 값, 거리 측정 방식 |
| 특징 | 느린 예측, 빠른 학습 |

---
